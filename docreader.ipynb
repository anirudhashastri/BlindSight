{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "from docx import Document\n",
    "import ollama\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "\n",
    "# Function to speak text\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available microphones\n",
    "def list_microphones():\n",
    "    mic_list = sr.Microphone.list_microphone_names()\n",
    "    print(\"Available microphones:\")\n",
    "    for i, microphone_name in enumerate(mic_list):\n",
    "        print(f\"{i}: {microphone_name}\")\n",
    "    return mic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select microphone\n",
    "def select_microphone():\n",
    "    mic_list = list_microphones()\n",
    "    try:\n",
    "        mic_index = int(input(\"Enter the microphone index you want to use: \"))\n",
    "        if mic_index >= 0 and mic_index < len(mic_list):\n",
    "            return mic_index\n",
    "        else:\n",
    "            print(\"Invalid index, using the default microphone.\")\n",
    "            return None\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, using the default microphone.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to voice commands\n",
    "def listen(mic_index=None):\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone(device_index=mic_index) if mic_index is not None else sr.Microphone()\n",
    "    \n",
    "    with mic as source:\n",
    "        print(\"Listening for command...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "        \n",
    "        try:\n",
    "            command = recognizer.recognize_google(audio)\n",
    "            print(f\"You said: {command}\")\n",
    "            return command.lower()\n",
    "        except sr.UnknownValueError:\n",
    "            speak(\"Sorry, I didn't catch that.\")\n",
    "            return \"\"\n",
    "        except sr.RequestError:\n",
    "            speak(\"Sorry, there was a network error.\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent recognition using LLM\n",
    "def recognize_intent_with_llm(command):\n",
    "    prompt = (\n",
    "        f\"Classify this command as one of the following::'add_text', 'delete_line', 'read_document', 'read_line', 'next_line', 'previous_line', 'edit_line', 'read_between_lines' , 'summarize' , 'exit'.Reply with only one word from this list, without any extra text, punctuation, or phrases, based on the command given after the colon:'{command}\"\n",
    "    )\n",
    "    # Send prompt to LLM\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    intent = \"\"\n",
    "    for chunk in stream:\n",
    "        intent += chunk['message']['content']\n",
    "    print(f\"Intent: {intent.strip().lower()}\")\n",
    "    return intent.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_range_with_llm(command):  \n",
    "\n",
    "    prompt = (\n",
    "        f\"User input: \\\"{command}\\\"\\n\"\n",
    "        \"Extract and return two numbers separated by a comma from the input, specifying line numbers or a range.- If a range is mentioned (e.g., 'lines 3 to 5' or 'read lines 2 through 4'), return the start and end numbers (e.g., '3,5' or '2,4').- If a single line is mentioned (e.g., 'line 7' or 'read line 10'), return the same number twice (e.g., '7,7' or '10,10').Provide only the two numbers separated by a comma and nothing else.\"\n",
    "        #\"- For inputs that don't mention any line numbers, return '0,0'.\\n\"\n",
    "    )\n",
    "    # Send prompt to LLM\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    line_range = \"\"\n",
    "    for chunk in stream:\n",
    "        line_range += chunk['message']['content']\n",
    "    \n",
    "    # Parse the LLM response\n",
    "    line_range = line_range.strip()\n",
    "    print(f\"Line range: {line_range}\")\n",
    "    try:\n",
    "        start_line, end_line = map(int, line_range.split(','))\n",
    "    except ValueError:\n",
    "        return None, None  # Return None if parsing fails\n",
    "    \n",
    "    return start_line, end_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_or_paragraph_range_with_llm(command):\n",
    "    prompt = (\n",
    "        f\"User input: \\\"{command}\\\"\\n\"\n",
    "        \"Determine whether the input refers to lines or paragraphs, and extract the numeric range.\\n\"\n",
    "        \"Respond with one of the following formats:\\n\"\n",
    "        \"- If the input specifies a range of lines (e.g., 'read lines 3 to 5'), respond with 'line:3,5'.\\n\"\n",
    "        \"- If the input specifies a single line (e.g., 'read line 7'), respond with 'line:7,7'.\\n\"\n",
    "        \"- If the input specifies a range of paragraphs (e.g., 'read paragraphs 2 to 4'), respond with 'paragraph:2,4'.\\n\"\n",
    "        \"- If the input specifies a single paragraph (e.g., 'paragraph 3'), respond with 'paragraph:3,3'.\\n\"\n",
    "        \"Provide only the specified output format and nothing else.\"\n",
    "    )\n",
    "    # Send prompt to LLM\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk['message']['content']\n",
    "    \n",
    "    # Parse the LLM response\n",
    "    response = response.strip()\n",
    "    print(f\"LLM Response: {response}\")\n",
    "    \n",
    "    try:\n",
    "        mode, range_values = response.split(\":\")\n",
    "        start, end = map(int, range_values.split(','))\n",
    "    except ValueError:\n",
    "        return None, None, None  # Return None if parsing fails\n",
    "    \n",
    "    if mode not in ['line', 'paragraph']:\n",
    "        return None, None, None  # Ensure mode is either 'line' or 'paragraph'\n",
    "    \n",
    "    return start, end, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and initialize document\n",
    "def open_document(filename):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        return Document(filename)\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        with open(filename, \"r+\") as file:\n",
    "            return file.read().splitlines()  # Treat each line as an entry in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save changes to the document\n",
    "def save_document(content, filename):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        content.save(filename)\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        with open(filename, \"w\") as file:\n",
    "            file.write(\"\\n\".join(content))  # Save content list back to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text to the document\n",
    "def add_text(content, filename, new_text):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        content.add_paragraph(new_text)\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        content.append(new_text)\n",
    "    speak(\"Text added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete text by line number\n",
    "def delete_line(content, line_num, filename):\n",
    "    try:\n",
    "        if filename.endswith(\".docx\"):\n",
    "            paragraph = content.paragraphs[line_num]\n",
    "            paragraph.clear()\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            content.pop(line_num)\n",
    "        speak(\"Line deleted successfully.\")\n",
    "    except IndexError:\n",
    "        speak(\"Invalid line number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LLM to edit a specific line\n",
    "def edit_line_with_llm(original_line, edit_instruction):\n",
    "    prompt = (\n",
    "        f\"Original line: \\\"{original_line}\\\"\\n\"\n",
    "        f\"Edit request: {edit_instruction}\\n\"\n",
    "        \"Provide only the updated line with no extra text:\"\n",
    "    )\n",
    "    # Send prompt to LLM\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.1:8b',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    edited_line = \"\"\n",
    "    for chunk in stream:\n",
    "        edited_line += chunk['message']['content']\n",
    "\n",
    "    return edited_line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(content, filename, line_start=None, line_end=None):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        # Extract paragraphs as a list of lines\n",
    "        lines = [p.text for p in content.paragraphs]\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        # Treat content as a list of lines\n",
    "        lines = content\n",
    "    \n",
    "    # Adjust for single-line reading if no range provided\n",
    "    if line_start is not None and line_end is None:\n",
    "        text_to_read = lines[line_start - 1]  # 1-based index\n",
    "    elif line_start is not None and line_end is not None:\n",
    "        text_to_read = \"\\n\".join(lines[line_start - 1:line_end])  # 1-based inclusive range\n",
    "    else:\n",
    "        text_to_read = \"\\n\".join(lines)  # Default to full document\n",
    "\n",
    "    speak(text_to_read)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def summarize_range(content, filename, start, end, paragraph_flag='line'):\n",
    "    \n",
    "    if paragraph_flag == None:\n",
    "        paragraph_flag = 'line'\n",
    "    \n",
    "    # Determine lines or paragraphs based on file type and mode\n",
    "    if filename.endswith(\".docx\"):\n",
    "        if paragraph_flag == 'paragraph':\n",
    "            # Extract paragraphs\n",
    "            items = [p.text for p in content.paragraphs if p.text.strip()]\n",
    "        else:\n",
    "            # Extract lines by splitting paragraphs\n",
    "            items = [line for p in content.paragraphs for line in p.text.split(\"\\n\")]\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        if paragraph_flag == 'paragraph':\n",
    "            # Split content into paragraphs\n",
    "            items = content.split(\"\\n\\n\")\n",
    "        else:\n",
    "            # Split content into lines\n",
    "            items = content.split(\"\\n\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .docx or .txt file.\")\n",
    "\n",
    "    # Validate range\n",
    "    if start < 1 or end > len(items):\n",
    "        return \"Invalid range specified.\"\n",
    "\n",
    "    # Extract the relevant range\n",
    "    selected_text = \"\\n\".join(items[start - 1:end])  # 1-based inclusive range\n",
    "\n",
    "    # Use the Llama model to summarize the selected text\n",
    "    prompt = f\"Summarize the following text:\\n\\n{selected_text}\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3.1:8b',\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        summary = \"\".join(chunk['message']['content'] for chunk in response)\n",
    "        return summary.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarizing: {e}\")\n",
    "        return f\"Error in summarizing: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main editing function\n",
    "def main():\n",
    "    # Select microphone\n",
    "    mic_index = select_microphone()\n",
    "    \n",
    "    # Hardcoded file path for testing\n",
    "    filename = r\"E:\\BlindSight\\test\\test.docx\"  # Replace with your test file path\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        speak(\"File not found. Please try again.\")\n",
    "        return\n",
    "\n",
    "    # Open the document\n",
    "    content = open_document(filename)\n",
    "    current_line = 0\n",
    "\n",
    "    speak(\"File opened successfully. You can say commands like add text, delete line, edit line, or read document.\")\n",
    "    \n",
    "    while True:\n",
    "        command = listen(mic_index)\n",
    "        intent = recognize_intent_with_llm(command)\n",
    "        \n",
    "        if intent == \"exit\":\n",
    "            save_document(content, filename)\n",
    "            speak(\"Changes saved. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        elif intent == \"add_text\":\n",
    "            speak(\"What text would you like to add?\")\n",
    "            new_text = listen(mic_index)\n",
    "            add_text(content, filename, new_text)\n",
    "\n",
    "        elif intent == \"delete_line\":\n",
    "            speak(\"Which line number would you like to delete?\")\n",
    "            try:\n",
    "                #line_num = int(listen(mic_index))\n",
    "                # TODO: add the line number detection here\n",
    "                line_num, _ , _= get_line_or_paragraph_range_with_llm(command)\n",
    "                delete_line(content, line_num - 1, filename)\n",
    "            except ValueError:\n",
    "                speak(\"Invalid line number.\")\n",
    "\n",
    "        elif intent == \"read_document\":\n",
    "            read_document(content, filename)\n",
    "\n",
    "        elif intent == \"read_line\":\n",
    "            #speak(\"Which line number would you like to hear?\")\n",
    "            try:\n",
    "                #line_num = int(listen(mic_index))\n",
    "                #read_document(content, filename, line_num - 1)\n",
    "\n",
    "                #line_command = listen(mic_index)\n",
    "    \n",
    "                # Use LLM to extract the line number from the user's command\n",
    "                line_num, _ , _= get_line_or_paragraph_range_with_llm(command)\n",
    "                print(line_num)\n",
    "                read_document(content, filename, line_start=line_num)\n",
    "            except ValueError:\n",
    "                speak(\"Invalid line number.\")\n",
    "        \n",
    "        elif intent == \"next_line\":\n",
    "            current_line += 1\n",
    "            read_document(content, filename, line_start=current_line)\n",
    "\n",
    "        elif intent == \"previous_line\":\n",
    "            current_line = max(0, current_line - 1)\n",
    "            read_document(content, filename, line_start=current_line)\n",
    "        \n",
    "        elif intent == \"edit_line\":\n",
    "            #speak(\"Which line number would you like to edit?\")\n",
    "            try:\n",
    "                line_command = listen(mic_index)\n",
    "    \n",
    "                # Use LLM to extract the line number from the user's command\n",
    "                line_num, _ , _= get_line_or_paragraph_range_with_llm(command)\n",
    "                \n",
    "                \n",
    "                #line_num = line_num - 1\n",
    "                original_line = content.paragraphs[line_num].text if filename.endswith(\".docx\") else content[line_num]\n",
    "                \n",
    "                speak(\"What changes would you like to make?\")\n",
    "                edit_instruction = listen(mic_index)\n",
    "                \n",
    "                # Get edited line from LLM\n",
    "                edited_line = edit_line_with_llm(original_line, edit_instruction)\n",
    "                \n",
    "                # Update the document with the edited line\n",
    "                if filename.endswith(\".docx\"):\n",
    "                    content.paragraphs[line_num].text = edited_line\n",
    "                elif filename.endswith(\".txt\"):\n",
    "                    content[line_num] = edited_line\n",
    "                \n",
    "                speak(\"Line edited successfully.\")\n",
    "            except (ValueError, IndexError):\n",
    "                speak(\"Invalid line number or edit command.\")\n",
    "        elif intent == \"summarize\":\n",
    "            #ask which paragraph to summarize\n",
    "            start , end , para = get_line_or_paragraph_range_with_llm(command)\n",
    "            summary = summarize_range(content, filename , start, end, paragraph_flag=para )\n",
    "            speak(summary)\n",
    "        elif intent == \"read_between_lines\":\n",
    "            #ask for the two lines\n",
    "            try:\n",
    "                start_line_num, end_line_num , paragraph = get_line_or_paragraph_range_with_llm(command)\n",
    "                print(\"i am in read between lines\")\n",
    "                print(start_line_num,end_line_num)\n",
    "                read_document(content, filename, line_start=start_line_num, line_end=end_line_num)\n",
    "            except ValueError:\n",
    "                speak(\"Invalid line numbers.\")\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            speak(\"Sorry, I didn't understand that command. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available microphones:\n",
      "0: Microsoft Sound Mapper - Input\n",
      "1: Microphone (3- USB PnP Audio De\n",
      "2: Microphone (G435 Wireless Gamin\n",
      "3: Headset Microphone (Oculus Virt\n",
      "4: Microphone (Realtek USB Audio)\n",
      "5: Microphone (Steam Streaming Mic\n",
      "6: Microsoft Sound Mapper - Output\n",
      "7: Headset Earphone (G435 Wireless\n",
      "8: Headphones (Realtek USB Audio)\n",
      "9: ASUS VG32V (NVIDIA High Definit\n",
      "10: Headphones (Oculus Virtual Audi\n",
      "11: Realtek Digital Output (Realtek\n",
      "12: Speakers (Steam Streaming Speak\n",
      "13: Speakers (Steam Streaming Micro\n",
      "14: U32J59x (NVIDIA High Definition\n",
      "15: Primary Sound Capture Driver\n",
      "16: Microphone (3- USB PnP Audio Device)\n",
      "17: Microphone (G435 Wireless Gaming Headset)\n",
      "18: Headset Microphone (Oculus Virtual Audio Device)\n",
      "19: Microphone (Realtek USB Audio)\n",
      "20: Microphone (Steam Streaming Microphone)\n",
      "21: Primary Sound Driver\n",
      "22: Headset Earphone (G435 Wireless Gaming Headset)\n",
      "23: Headphones (Realtek USB Audio)\n",
      "24: ASUS VG32V (NVIDIA High Definition Audio)\n",
      "25: Headphones (Oculus Virtual Audio Device)\n",
      "26: Realtek Digital Output (Realtek USB Audio)\n",
      "27: Speakers (Steam Streaming Speakers)\n",
      "28: Speakers (Steam Streaming Microphone)\n",
      "29: U32J59x (NVIDIA High Definition Audio)\n",
      "30: Headphones (Realtek USB Audio)\n",
      "31: ASUS VG32V (NVIDIA High Definition Audio)\n",
      "32: Headphones (Oculus Virtual Audio Device)\n",
      "33: Realtek Digital Output (Realtek USB Audio)\n",
      "34: Headset Earphone (G435 Wireless Gaming Headset)\n",
      "35: Speakers (Steam Streaming Speakers)\n",
      "36: Speakers (Steam Streaming Microphone)\n",
      "37: U32J59x (NVIDIA High Definition Audio)\n",
      "38: Microphone (G435 Wireless Gaming Headset)\n",
      "39: Headset Microphone (Oculus Virtual Audio Device)\n",
      "40: Microphone (Realtek USB Audio)\n",
      "41: Microphone (3- USB PnP Audio Device)\n",
      "42: Microphone (Steam Streaming Microphone)\n",
      "43: Microphone (USB PnP Audio Device)\n",
      "44: Output 1 (OCULUSVAD Wave Speaker Headphone)\n",
      "45: Output 2 (OCULUSVAD Wave Speaker Headphone)\n",
      "46: Input (OCULUSVAD Wave Speaker Headphone)\n",
      "47: Headset Microphone (OCULUSVAD Wave Microphone Headphone)\n",
      "48: Microphone (G435 Wireless Gaming Headset)\n",
      "49: Headset Earphone (G435 Wireless Gaming Headset)\n",
      "50: Output (NVIDIA High Definition Audio)\n",
      "51: Output ()\n",
      "52: Line In (Realtek USB Audio)\n",
      "53: Microphone (Realtek USB Audio)\n",
      "54: Stereo Mix (Realtek USB Audio)\n",
      "55: Headphones (Realtek USB Audio)\n",
      "56: Speakers (Realtek USB Audio)\n",
      "57: SPDIF Interface (Realtek USB Audio)\n",
      "58: Headset Earphone (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Big bro))\n",
      "59: Headset Microphone (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(Big bro))\n",
      "60: Microphone (Steam Streaming Microphone Wave)\n",
      "61: Speakers (Steam Streaming Microphone Wave)\n",
      "62: Input (Steam Streaming Speakers Wave)\n",
      "63: Speakers (Steam Streaming Speakers Wave)\n",
      "64: Headset Earphone (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(small bro))\n",
      "65: Headset Microphone (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free%0\n",
      ";(small bro))\n",
      "66: Speakers ()\n",
      "67: Speakers ()\n",
      "Listening for command...\n",
      "You said: second paragraph\n",
      "Intent: next_line\n",
      "Listening for command...\n",
      "You said: summarize second paragraph\n",
      "Intent: summarize\n",
      "LLM Response: paragraph:2,2\n",
      "Listening for command...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[89], line 20\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m speak(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile opened successfully. You can say commands like add text, delete line, edit line, or read document.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     command \u001b[38;5;241m=\u001b[39m \u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmic_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     intent \u001b[38;5;241m=\u001b[39m recognize_intent_with_llm(command)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m intent \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[1;32mIn[77], line 12\u001b[0m, in \u001b[0;36mlisten\u001b[1;34m(mic_index)\u001b[0m\n\u001b[0;32m      9\u001b[0m audio \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mlisten(source)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     command \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_google\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou said: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m command\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\AIHCI\\lib\\site-packages\\speech_recognition\\recognizers\\google.py:256\u001b[0m, in \u001b[0;36mrecognize_legacy\u001b[1;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[0m\n\u001b[0;32m    251\u001b[0m request_builder \u001b[38;5;241m=\u001b[39m create_request_builder(\n\u001b[0;32m    252\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint, key\u001b[38;5;241m=\u001b[39mkey, language\u001b[38;5;241m=\u001b[39mlanguage, filter_level\u001b[38;5;241m=\u001b[39mpfilter\n\u001b[0;32m    253\u001b[0m )\n\u001b[0;32m    254\u001b[0m request \u001b[38;5;241m=\u001b[39m request_builder\u001b[38;5;241m.\u001b[39mbuild(audio_data)\n\u001b[1;32m--> 256\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[43mobtain_transcription\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation_timeout\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m OutputParser(\n\u001b[0;32m    261\u001b[0m     show_all\u001b[38;5;241m=\u001b[39mshow_all, with_confidence\u001b[38;5;241m=\u001b[39mwith_confidence\n\u001b[0;32m    262\u001b[0m )\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_parser\u001b[38;5;241m.\u001b[39mparse(response_text)\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\AIHCI\\lib\\site-packages\\speech_recognition\\recognizers\\google.py:222\u001b[0m, in \u001b[0;36mobtain_transcription\u001b[1;34m(request, timeout)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecognition connection failed: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e\u001b[38;5;241m.\u001b[39mreason)\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\AIHCI\\lib\\http\\client.py:460\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\AIHCI\\lib\\http\\client.py:583\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 583\u001b[0m         chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chunk_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    585\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\AIHCI\\lib\\http\\client.py:566\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 566\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_next_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\AIHCI\\lib\\http\\client.py:526\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aniru\\anaconda3\\envs\\AIHCI\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIHCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
